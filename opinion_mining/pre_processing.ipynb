{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "#### First we analyze the data:\n",
    "1. Search for missing values\n",
    "2. Check dataset balance\n",
    "#### Then we use NLP techniques such as:\n",
    "1. Stemming\n",
    "2. Tokenization\n",
    "3. Stop-word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from matplotlib import pyplot\n",
    "from utils.phrase_breaker import phrase_breaker\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data onto dataframe\n",
    "data = pd.read_csv(\"data/reviews.csv\", names=[\"Review Text\", \"Stars\", \"Polarity\"])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for null values\n",
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset balance\n",
    "sns.countplot(x=\"Stars\", data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K most frequent words\n",
    "counter = Counter(\" \".join(data[\"Review Text\"]).split())\n",
    "most_occur = counter.most_common(25)\n",
    "words, freq = zip(*most_occur)\n",
    "\n",
    "freq_words = pd.DataFrame({'Frequency': freq, 'Word': words})\n",
    "\n",
    "fig, ax = pyplot.subplots(figsize=(15.0, 5.0))\n",
    "sns.lineplot(data=freq_words, x=\"Word\", y=\"Frequency\", ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe with original vocabulary (existent words)\n",
    "# Mainly for performance evaluation purposes\n",
    "\n",
    "original_set = set()\n",
    "data[\"Review Text\"].str.split().apply(original_set.update)\n",
    "\n",
    "original_data = {\n",
    "    'Number of Words': [len(list(original_set))],\n",
    "    'Type': [\"original\"],\n",
    "}\n",
    "\n",
    "pf_df = pd.DataFrame(original_data)\n",
    "pf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying phrase breaker to reviews\n",
    "data[\"Review Text\"] = data[\"Review Text\"].apply(phrase_breaker)\n",
    "data[\"Review Text\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the decrease in complexity levels after separating every word in every review\n",
    "word_sep_set = set()\n",
    "data[\"Review Text\"].str.split().apply(word_sep_set.update)\n",
    "\n",
    "word_sep_data = {\n",
    "    'Number of Words': len(list(word_sep_set)),\n",
    "    'Type': \"word_separation\",\n",
    "}\n",
    "\n",
    "pf_df = pf_df.append(word_sep_data, ignore_index=True)\n",
    "\n",
    "sns.barplot(x=\"Type\", y=\"Number of Words\", data=pf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize all reviews for pre-processing purposes\n",
    "data[\"Review Text\"] = data[\"Review Text\"].apply(word_tokenize)\n",
    "data[\"Review Text\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS Tagging\n",
    "pos_tagged_vocab = data[\"Review Text\"].apply(pos_tag)\n",
    "pos_tagged_vocab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_categories(pos_tagged_words):\n",
    "    pos_tags_filter = [\"JJ\", \"JJR\", \"JJS\", \"NN\", \"NNS\", \"RB\", \"RBR\", \"RBS\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]\n",
    "    selected_words = []\n",
    "    for word, pos_tag in pos_tagged_words:\n",
    "        if pos_tag in pos_tags_filter:\n",
    "            selected_words.append(word)\n",
    "    return selected_words\n",
    "\n",
    "data[\"Review Text\"] = pos_tagged_vocab.apply(select_categories)\n",
    "data[\"Review Text\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the decrease in complexity levels after filtering data through POS Tagging\n",
    "pos_tagging_set = set()\n",
    "data[\"Review Text\"].apply(pos_tagging_set.update)\n",
    "\n",
    "pos_tagging_data = {\n",
    "    'Number of Words': len(list(pos_tagging_set)),\n",
    "    'Type': \"pos_tagging\",\n",
    "}\n",
    "\n",
    "pf_df = pf_df.append(pos_tagging_data, ignore_index=True)\n",
    "\n",
    "sns.barplot(x=\"Type\", y=\"Number of Words\", data=pf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Processing with lower casing and lemmatization to reduce complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All review text is transformed into lower case\n",
    "def lower_casing(words):\n",
    "    lower_case_words = []\n",
    "    for word in words:\n",
    "        lower_case_words.append(word.lower())\n",
    "    return lower_case_words\n",
    "\n",
    "data[\"Review Text\"] = data[\"Review Text\"].apply(lower_casing)\n",
    "data[\"Review Text\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the decrease in complexity levels after lower casing every review\n",
    "lower_case = set()\n",
    "data[\"Review Text\"].apply(lower_case.update)\n",
    "\n",
    "lower_case_data = {\n",
    "    'Number of Words': len(list(lower_case)),\n",
    "    'Type': \"lower_case\",\n",
    "}\n",
    "\n",
    "pf_df = pf_df.append(lower_case_data, ignore_index=True)\n",
    "\n",
    "sns.barplot(x=\"Type\", y=\"Number of Words\", data=pf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatization(sentence):\n",
    "    lemmatized_sentence = []\n",
    "\n",
    "    for word in sentence:\n",
    "        lemmatized_sentence.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "    \n",
    "    return lemmatized_sentence\n",
    "\n",
    "data[\"Review Text\"] = data[\"Review Text\"].apply(lemmatization)\n",
    "data[\"Review Text\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the decrease in complexity levels after lemmatization\n",
    "lemmatization = set()\n",
    "data[\"Review Text\"].apply(lemmatization.update)\n",
    "\n",
    "lemmatization_data = {\n",
    "    'Number of Words': len(list(lemmatization)),\n",
    "    'Type': \"lemmatization\",\n",
    "}\n",
    "\n",
    "pf_df = pf_df.append(lemmatization_data, ignore_index=True)\n",
    "\n",
    "sns.barplot(x=\"Type\", y=\"Number of Words\", data=pf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Review Text'] = data[\"Review Text\"].apply(\" \".join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Share data between notebooks\n",
    "data.to_csv('data/filtered_reviews.csv', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2f225c214e61cba03683fbc55c8c48645202e001eb1798a869794811d95e1b14"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "2f225c214e61cba03683fbc55c8c48645202e001eb1798a869794811d95e1b14"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
